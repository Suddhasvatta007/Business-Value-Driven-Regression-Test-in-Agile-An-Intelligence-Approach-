{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kxtUNnk0Bvx"
      },
      "source": [
        "# Multi-Cycle Experiment Pipeline: Hist-RTSGA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVZyvDu90Bv0",
        "outputId": "0fc45493-6eb4-41c8-e630-4800f49486cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Setup complete. Ready for multi-cycle experiments.\n",
            "Configuration: 20 cycles, 30 runs per cycle\n",
            "RTW ratios: ['5.0%']\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl\n",
        "# ======================================================\n",
        "# Cell 1: Imports and Setup\n",
        "# ======================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Experiment Configuration\n",
        "RNG_SEED = 42\n",
        "random.seed(RNG_SEED)\n",
        "np.random.seed(RNG_SEED)\n",
        "\n",
        "NUM_CYCLES = 20\n",
        "RUNS_PER_CYCLE = 30\n",
        "RTW_RATIOS = [0.05]  # 5% and 10%\n",
        "\n",
        "# GA Parameters\n",
        "GA_PARAMS = {\n",
        "    'max_generations': 100,\n",
        "    'crossover_prob': 0.8,\n",
        "    'mutation_prob': 0.05,\n",
        "    'population_size': 10\n",
        "}\n",
        "\n",
        "print(\"Setup complete. Ready for multi-cycle experiments.\")\n",
        "print(f\"Configuration: {NUM_CYCLES} cycles, {RUNS_PER_CYCLE} runs per cycle\")\n",
        "print(f\"RTW ratios: {[f'{r*100}%' for r in RTW_RATIOS]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqgyp9IH0Bv1",
        "outputId": "4d9018b4-4a23-498a-e30d-db717f9f2f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 40 tests, 124 requirements\n",
            "Total execution time: 349.65\n",
            "Requirements range: 1 to 124\n",
            "Business values range: 2 to 89\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# Cell 2: Data Loading and Preparation\n",
        "# ======================================================\n",
        "\n",
        "# Load dataset\n",
        "filename = '/content/drive/MyDrive/RTS/Dataset/mapped_dataset_1-half.xlsx'  # Update this path as needed\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# Normalize column names\n",
        "df.columns = [col.strip().lower() for col in df.columns]\n",
        "\n",
        "# Create data maps\n",
        "data_maps = {\n",
        "    'req_to_tests': df.groupby('us_id')['tc_id'].apply(set).to_dict(),\n",
        "    'req_to_bv': df.groupby('us_id')['us_businessvalue'].first().to_dict(),\n",
        "    'test_to_time': df.groupby('tc_id')['tc_executiontime'].first().to_dict(),\n",
        "    'all_req_ids': sorted(df['us_id'].unique()),\n",
        "    'tests': sorted(df['tc_id'].unique())\n",
        "}\n",
        "\n",
        "# For BCPSO: test to requirements mapping\n",
        "data_maps['test_to_reqs'] = df.groupby('tc_id')['us_id'].apply(set).to_dict()\n",
        "\n",
        "data_maps['n_reqs'] = len(data_maps['all_req_ids'])\n",
        "data_maps['n_tests'] = len(data_maps['tests'])\n",
        "\n",
        "# Calculate total execution time\n",
        "TOTAL_EXEC_TIME = df.drop_duplicates(subset=['tc_id'])['tc_executiontime'].sum()\n",
        "\n",
        "print(f\"Dataset loaded: {data_maps['n_tests']} tests, {data_maps['n_reqs']} requirements\")\n",
        "print(f\"Total execution time: {TOTAL_EXEC_TIME:.2f}\")\n",
        "print(f\"Requirements range: {min(data_maps['all_req_ids'])} to {max(data_maps['all_req_ids'])}\")\n",
        "print(f\"Business values range: {min(data_maps['req_to_bv'].values())} to {max(data_maps['req_to_bv'].values())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB-Z2itU0Bv2",
        "outputId": "de8f13ed-20dc-476b-dad5-4da352a1034d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Algorithms updated: Hist-RTSGA now tracks starvation.\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# Cell 3: Algorithm Implementations (Updated with Starvation Logic)\n",
        "# ======================================================\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# The GA Logic as a Reusable Function (Updated)\n",
        "# ======================================================\n",
        "def run_ga_instance(max_exec_time, ga_params, data_maps):\n",
        "    # Unpack GA parameters and data maps\n",
        "    max_generations = ga_params['max_generations']\n",
        "    crossover_prob = ga_params['crossover_prob']\n",
        "    mutation_prob = ga_params['mutation_prob']\n",
        "\n",
        "    req_to_tests = data_maps['req_to_tests']\n",
        "    req_to_bv = data_maps['req_to_bv']\n",
        "    test_to_time = data_maps['test_to_time']\n",
        "    all_req_ids = data_maps['all_req_ids']\n",
        "    # bval_to_reqs_map = data_maps['bval_to_reqs_map']  # kept for compatibility if needed\n",
        "    n_requirements = len(all_req_ids)\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # New: starvation-related configuration (all optional)\n",
        "    # ----------------------------------------------------------------------\n",
        "    # req_to_starvation: requirement -> starvation counter (cycles since last covered)\n",
        "    req_to_starvation = data_maps.get('req_to_starvation', {})\n",
        "    # bv_tolerance: fraction of best BV we are willing to accept (e.g., 0.9 = 90%)\n",
        "    bv_tolerance = ga_params.get('bv_tolerance', 1.0)\n",
        "    # starvation_weight: strength of starvation bonus when BV is close to best\n",
        "    starvation_weight = ga_params.get('starvation_weight', 0.0)\n",
        "\n",
        "    # Track best BV seen (estimated baseline) to apply tolerance inside fitness\n",
        "    baseline_tracker = {'best_bv_seen': 0.0}\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Helper: Evaluation Decomposition (Deb, 2000)\n",
        "    # Source: Deb (2000), \"An Efficient Constraint Handling Method for GAs\"\n",
        "    # ----------------------------------------------------------------------\n",
        "    def evaluate_chrom(chrom):\n",
        "        covered_tests = set()\n",
        "        total_bv = 0\n",
        "        for idx, val in enumerate(chrom):\n",
        "            if val:\n",
        "                req = all_req_ids[idx]\n",
        "                covered_tests |= req_to_tests.get(req, set())\n",
        "                total_bv += req_to_bv.get(req, 0)\n",
        "        total_time = sum(test_to_time.get(t, 0) for t in covered_tests)\n",
        "        feasible = (total_time <= max_exec_time)\n",
        "        violation = max(0, total_time - max_exec_time)\n",
        "        return total_bv, total_time, feasible, violation\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # New: starvation relief score\n",
        "    # ----------------------------------------------------------------------\n",
        "    def starvation_relief(chrom):\n",
        "        \"\"\"\n",
        "        Simple starvation relief: sum of starvation counters\n",
        "        for all selected requirements. Higher = more relief.\n",
        "        \"\"\"\n",
        "        if not req_to_starvation:\n",
        "            return 0.0\n",
        "        relief_score = 0.0\n",
        "        for idx, val in enumerate(chrom):\n",
        "            if val:\n",
        "                req = all_req_ids[idx]\n",
        "                relief_score += float(req_to_starvation.get(req, 0.0))\n",
        "        return relief_score\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Feasible Initialization (Chu & Beasley, 1998)\n",
        "    # ----------------------------------------------------------------------\n",
        "    def feasible_chromosome():\n",
        "        chrom = [0] * n_requirements\n",
        "        indices = list(range(n_requirements))\n",
        "        random.shuffle(indices)\n",
        "        covered_tests = set()\n",
        "        total_time = 0\n",
        "        for idx in indices:\n",
        "            req = all_req_ids[idx]\n",
        "            tests = req_to_tests.get(req, set())\n",
        "            new_tests = tests - covered_tests\n",
        "            additional_time = sum(test_to_time.get(t, 0) for t in new_tests)\n",
        "            if total_time + additional_time <= max_exec_time:\n",
        "                chrom[idx] = 1\n",
        "                covered_tests |= new_tests\n",
        "                total_time += additional_time\n",
        "        return chrom\n",
        "\n",
        "    population_size = ga_params.get('population_size', 10)\n",
        "    population = [feasible_chromosome() for _ in range(population_size)]\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Greedy Seeding for Budgeted BV Coverage (Khuller, Moss, Naor, 1999)\n",
        "    # Source: Khuller–Moss–Naor (1999), Budgeted Maximum Coverage (1−1/e)\n",
        "    # ----------------------------------------------------------------------\n",
        "    def greedy_seed_variant():\n",
        "        chrom = [0] * n_requirements\n",
        "        covered = set()\n",
        "        used_time = 0\n",
        "        remaining = set(range(n_requirements))\n",
        "        while remaining:\n",
        "            best_idx, best_score = None, float(\"-inf\")\n",
        "            for idx in remaining:\n",
        "                req = all_req_ids[idx]\n",
        "                tests = req_to_tests.get(req, set())\n",
        "                new_tests = tests - covered\n",
        "                add_time = sum(test_to_time.get(t, 0) for t in new_tests)\n",
        "                if add_time < 0:\n",
        "                    continue\n",
        "                score = req_to_bv.get(req, 0) if add_time == 0 else (\n",
        "                    req_to_bv.get(req, 0) / (add_time + 1e-12)\n",
        "                )\n",
        "                if used_time + add_time <= max_exec_time and score > best_score:\n",
        "                    best_idx, best_score = idx, score\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            req = all_req_ids[best_idx]\n",
        "            new_tests = req_to_tests.get(req, set()) - covered\n",
        "            add_time = sum(test_to_time.get(t, 0) for t in new_tests)\n",
        "            if used_time + add_time <= max_exec_time:\n",
        "                chrom[best_idx] = 1\n",
        "                covered |= new_tests\n",
        "                used_time += add_time\n",
        "            remaining.remove(best_idx)\n",
        "        return chrom\n",
        "\n",
        "    # Inject a few seeds to improve the starting population\n",
        "    num_seeds = min(3, population_size)\n",
        "    for i in range(num_seeds):\n",
        "        population[i] = greedy_seed_variant()\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Value-Aware Repair / Improve Operator\n",
        "    # Source: Chu & Beasley (1998); randomized/efficiency-based repair literature\n",
        "    # ----------------------------------------------------------------------\n",
        "    def repair_value_aware(chrom):\n",
        "        chrom = chrom[:]  # work on a copy\n",
        "\n",
        "        # DROP phase: remove the least efficient requirement until feasible\n",
        "        while True:\n",
        "            _, _, feasible, _ = evaluate_chrom(chrom)\n",
        "            if feasible:\n",
        "                break\n",
        "            selected = [i for i, v in enumerate(chrom) if v]\n",
        "            if not selected:\n",
        "                break\n",
        "            worst_idx, worst_ratio = None, float(\"inf\")\n",
        "            for idx in selected:\n",
        "                req = all_req_ids[idx]\n",
        "                covered_other = set()\n",
        "                for j, v in enumerate(chrom):\n",
        "                    if v and j != idx:\n",
        "                        covered_other |= req_to_tests.get(all_req_ids[j], set())\n",
        "                unique_tests = req_to_tests.get(req, set()) - covered_other\n",
        "                time_save = sum(test_to_time.get(t, 0) for t in unique_tests)\n",
        "                bv_loss = req_to_bv.get(req, 0)\n",
        "                ratio = bv_loss / (time_save + 1e-12) if time_save > 0 else float(\"inf\")\n",
        "                ratio += random.uniform(-1e-9, 1e-9)  # tie-breaking noise\n",
        "                if ratio < worst_ratio:\n",
        "                    worst_ratio, worst_idx = ratio, idx\n",
        "            if worst_idx is None:\n",
        "                break\n",
        "            chrom[worst_idx] = 0\n",
        "\n",
        "        # ADD phase: greedily add the most efficient requirement that fits\n",
        "        while True:\n",
        "            _, tt, _, _ = evaluate_chrom(chrom)\n",
        "            slack = max_exec_time - tt\n",
        "            if slack <= 0:\n",
        "                break\n",
        "            candidates = [i for i, v in enumerate(chrom) if not v]\n",
        "            best_idx, best_score = None, float(\"-inf\")\n",
        "            covered_now = set()\n",
        "            for j, v in enumerate(chrom):\n",
        "                if v:\n",
        "                    covered_now |= req_to_tests.get(all_req_ids[j], set())\n",
        "            for idx in candidates:\n",
        "                req = all_req_ids[idx]\n",
        "                new_tests = req_to_tests.get(req, set()) - covered_now\n",
        "                add_time = sum(test_to_time.get(t, 0) for t in new_tests)\n",
        "                if add_time < 0:\n",
        "                    continue\n",
        "                score = req_to_bv.get(req, 0) if add_time == 0 else (\n",
        "                    req_to_bv.get(req, 0) / (add_time + 1e-12)\n",
        "                )\n",
        "                score += random.uniform(-1e-9, 1e-9)\n",
        "                if add_time <= slack and score > best_score:\n",
        "                    best_idx, best_score = idx, score\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chrom[best_idx] = 1\n",
        "\n",
        "        return chrom\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Fitness Function (Yoo & Harman, 2012, Sec. 5.1) + starvation extension\n",
        "    # ----------------------------------------------------------------------\n",
        "    def fitness(chrom):\n",
        "        total_bv, _, _, _ = evaluate_chrom(chrom)\n",
        "\n",
        "        # If no starvation handling configured, behave like original GA.\n",
        "        if starvation_weight <= 0.0:\n",
        "            return total_bv\n",
        "\n",
        "        best_bv_seen = baseline_tracker['best_bv_seen']\n",
        "\n",
        "        # Only reward starvation when this chromosome's BV is close to best BV.\n",
        "        if best_bv_seen > 0.0 and bv_tolerance < 1.0:\n",
        "            min_acceptable_bv = bv_tolerance * best_bv_seen\n",
        "            if total_bv < min_acceptable_bv:\n",
        "                # Too far from best BV: ignore starvation bonus.\n",
        "                return total_bv\n",
        "\n",
        "        relief_score = starvation_relief(chrom)\n",
        "        # Combined score: BV is primary, starvation_relief is secondary.\n",
        "        return total_bv + starvation_weight * relief_score\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Deb-Style Tournament Selection (Deb, 2000)\n",
        "    # ----------------------------------------------------------------------\n",
        "    def deb_better(a_chrom, b_chrom):\n",
        "        # Evaluate both chromosomes\n",
        "        a_bv, _, a_feas, a_viol = evaluate_chrom(a_chrom)\n",
        "        b_bv, _, b_feas, b_viol = evaluate_chrom(b_chrom)\n",
        "\n",
        "        # Feasibility first\n",
        "        if a_feas and not b_feas:\n",
        "            return True\n",
        "        if b_feas and not a_feas:\n",
        "            return False\n",
        "\n",
        "        if a_feas and b_feas:\n",
        "            # Both feasible: compare by combined fitness (BV + starvation)\n",
        "            return fitness(a_chrom) > fitness(b_chrom)\n",
        "\n",
        "        # Both infeasible: lower violation is better\n",
        "        return a_viol < b_viol\n",
        "\n",
        "    def deb_tournament_selection(population, k=3):\n",
        "        contestants = random.sample(population, k=min(k, len(population)))\n",
        "        best = contestants[0]\n",
        "        for c in contestants[1:]:\n",
        "            if deb_better(c, best):\n",
        "                best = c\n",
        "        return best\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Crossover and Mutation Operators (Mitchell, 1998) - Unchanged\n",
        "    # ----------------------------------------------------------------------\n",
        "    def single_point_crossover(parent1, parent2):\n",
        "        if n_requirements < 2:\n",
        "            return parent1[:], parent2[:]\n",
        "        point = random.randint(1, n_requirements - 1)\n",
        "        child1 = parent1[:point] + parent2[point:]\n",
        "        child2 = parent2[:point] + parent1[point:]\n",
        "        return child1, child2\n",
        "\n",
        "    def mutate(chromosome, mutation_prob=mutation_prob):\n",
        "        if random.random() < mutation_prob and n_requirements > 0:\n",
        "            mutation_idx = random.randint(0, n_requirements - 1)\n",
        "            chromosome[mutation_idx] = 1 - chromosome[mutation_idx]\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Initialize baseline best BV from the initial population\n",
        "    # ----------------------------------------------------------------------\n",
        "    if population:\n",
        "        initial_bvs = [evaluate_chrom(ch)[0] for ch in population]\n",
        "        if initial_bvs:\n",
        "            baseline_tracker['best_bv_seen'] = max(initial_bvs)\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Main GA Loop (modified to use new operators + baseline tracking)\n",
        "    # ----------------------------------------------------------------------\n",
        "    for generation in range(max_generations):\n",
        "        # --- Selection ---\n",
        "        mating_pool = []\n",
        "        for _ in range(population_size // 2):\n",
        "            parent1 = deb_tournament_selection(population)\n",
        "            parent2 = deb_tournament_selection(population)\n",
        "            mating_pool.append((parent1, parent2))\n",
        "\n",
        "        # --- Variation & Repair ---\n",
        "        offspring = []\n",
        "        for parent1, parent2 in mating_pool:\n",
        "            if random.random() < crossover_prob:\n",
        "                child1, child2 = single_point_crossover(parent1, parent2)\n",
        "            else:\n",
        "                child1, child2 = parent1[:], parent2[:]\n",
        "            mutate(child1)\n",
        "            mutate(child2)\n",
        "            child1 = repair_value_aware(child1)\n",
        "            child2 = repair_value_aware(child2)\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # --- Replacement (using Deb's rules + combined fitness for ranking) ---\n",
        "        combined_population = population + offspring\n",
        "\n",
        "        def deb_key(ch):\n",
        "            total_bv, _, feas, viol = evaluate_chrom(ch)\n",
        "            if feas:\n",
        "                # Feasible: prioritize lower violation, then higher combined fitness\n",
        "                return (0, viol, -fitness(ch))\n",
        "            # Infeasible: prioritize lower violation, then higher BV\n",
        "            return (1, viol, -total_bv)\n",
        "\n",
        "        combined_population.sort(key=deb_key)\n",
        "        population = combined_population[:population_size]\n",
        "\n",
        "        # Update best BV seen so far (for BV tolerance)\n",
        "        best_bv_gen, _, _, _ = evaluate_chrom(population[0])\n",
        "        if best_bv_gen > baseline_tracker['best_bv_seen']:\n",
        "            baseline_tracker['best_bv_seen'] = best_bv_gen\n",
        "\n",
        "    # Final best solution is the top of the population after the last sort\n",
        "    best_solution = population[0]\n",
        "    best_fitness, _, _, _ = evaluate_chrom(best_solution)  # this is pure BV\n",
        "    selected_reqs = [all_req_ids[i] for i, val in enumerate(best_solution) if val]\n",
        "\n",
        "    return selected_reqs, best_fitness\n",
        "\n",
        "# ----- HISTORY-AWARE RTSGA (Updated to calculate Starvation) -----\n",
        "def run_hist_rtsga_cycles(rtw_ratio, data_maps, ga_params, num_cycles=20, runs_per_cycle=30):\n",
        "    \"\"\"Run history-aware RTSGA with starvation tracking and sqrt penalty\"\"\"\n",
        "    max_exec_time = rtw_ratio * TOTAL_EXEC_TIME\n",
        "    req_to_bv_orig = data_maps['req_to_bv']\n",
        "    all_req_ids = data_maps['all_req_ids']\n",
        "\n",
        "    # Initialize selection counts (for SQRT penalty)\n",
        "    select_counts = {req: 0 for req in req_to_bv_orig}\n",
        "\n",
        "    # Initialize starvation counts (Cycles since last selected)\n",
        "    # Initially 0 (or you could start higher to encourage early exploration)\n",
        "    starvation_counts = {req: 0 for req in all_req_ids}\n",
        "\n",
        "    cycles_results = []\n",
        "\n",
        "    for cycle_idx in range(1, num_cycles + 1):\n",
        "        # 1. Apply SQRT PENALTY to BV based on selection history\n",
        "        req_to_bv_for_cycle = {\n",
        "            req: req_to_bv_orig[req] / np.sqrt(1 + select_counts[req])\n",
        "            for req in req_to_bv_orig\n",
        "        }\n",
        "\n",
        "        # 2. Prepare Data Maps (include starvation counts)\n",
        "        data_maps_for_cycle = data_maps.copy()\n",
        "        data_maps_for_cycle['req_to_bv'] = req_to_bv_for_cycle\n",
        "        data_maps_for_cycle['req_to_starvation'] = starvation_counts.copy()\n",
        "\n",
        "        # Run multiple independent runs for this cycle\n",
        "        all_runs = []\n",
        "        for run_idx in range(runs_per_cycle):\n",
        "            selected_reqs, best_fitness = run_ga_instance(\n",
        "                max_exec_time=max_exec_time,\n",
        "                ga_params=ga_params,\n",
        "                data_maps=data_maps_for_cycle\n",
        "            )\n",
        "            all_runs.append({\n",
        "                'selected_reqs': selected_reqs,\n",
        "                'fitness': best_fitness\n",
        "            })\n",
        "\n",
        "        # Find best run\n",
        "        best_run = max(all_runs, key=lambda r: r['fitness'])\n",
        "\n",
        "        cycles_results.append({\n",
        "            'cycle_idx': cycle_idx,\n",
        "            'best_selected_reqs': best_run['selected_reqs'],\n",
        "            'best_bv': best_run['fitness'],\n",
        "            'all_runs': all_runs\n",
        "        })\n",
        "\n",
        "        # Update History for next cycle\n",
        "        selected_set = set(best_run['selected_reqs'])\n",
        "\n",
        "        for req in all_req_ids:\n",
        "            if req in selected_set:\n",
        "                # Selected: Reset starvation, increment selection count\n",
        "                select_counts[req] += 1\n",
        "                starvation_counts[req] = 0\n",
        "            else:\n",
        "                # Not selected: Increment starvation count\n",
        "                starvation_counts[req] += 1\n",
        "\n",
        "    return cycles_results\n",
        "\n",
        "# ----- STANDARD RTSGA (No History) -----\n",
        "def run_rtsga_cycles(rtw_ratio, data_maps, ga_params, num_cycles=20, runs_per_cycle=30):\n",
        "    \"\"\"Run standard RTSGA without history awareness\"\"\"\n",
        "    max_exec_time = rtw_ratio * TOTAL_EXEC_TIME\n",
        "\n",
        "    cycles_results = []\n",
        "\n",
        "    for cycle_idx in range(1, num_cycles + 1):\n",
        "        # No BV modification - use original values\n",
        "        all_runs = []\n",
        "        for run_idx in range(runs_per_cycle):\n",
        "            selected_reqs, best_fitness = run_ga_instance(\n",
        "                max_exec_time=max_exec_time,\n",
        "                ga_params=ga_params,\n",
        "                data_maps=data_maps\n",
        "            )\n",
        "            all_runs.append({\n",
        "                'selected_reqs': selected_reqs,\n",
        "                'fitness': best_fitness\n",
        "            })\n",
        "\n",
        "        best_run = max(all_runs, key=lambda r: r['fitness'])\n",
        "        cycles_results.append({\n",
        "            'cycle_idx': cycle_idx,\n",
        "            'best_selected_reqs': best_run['selected_reqs'],\n",
        "            'best_bv': best_run['fitness'],\n",
        "            'all_runs': all_runs\n",
        "        })\n",
        "\n",
        "    return cycles_results\n",
        "\n",
        "print(\"Algorithms updated: Hist-RTSGA now tracks starvation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMAFl6PS0Bv3",
        "outputId": "dae7a8ee-f6bd-49a1-d51d-b6fdae33bf0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing checkpoint directory: /content/drive/MyDrive/Starvation_Checkpoints1\n",
            "Starting Starvation Sensitivity Experiments at RTW=5.0%...\n",
            "Cycles: 20, Runs per Cycle: 30\n",
            "\n",
            "--- Checking Config: Tol0.9_Wt0.05 ---\n",
            "Found checkpoint for Tol0.9_Wt0.05. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.9_Wt0.1 ---\n",
            "Found checkpoint for Tol0.9_Wt0.1. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.9_Wt0.15 ---\n",
            "Found checkpoint for Tol0.9_Wt0.15. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.9_Wt0.2 ---\n",
            "Found checkpoint for Tol0.9_Wt0.2. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.9_Wt0.25 ---\n",
            "Found checkpoint for Tol0.9_Wt0.25. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.8_Wt0.05 ---\n",
            "Found checkpoint for Tol0.8_Wt0.05. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.8_Wt0.1 ---\n",
            "Found checkpoint for Tol0.8_Wt0.1. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.8_Wt0.15 ---\n",
            "Found checkpoint for Tol0.8_Wt0.15. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.8_Wt0.2 ---\n",
            "Found checkpoint for Tol0.8_Wt0.2. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.8_Wt0.25 ---\n",
            "Found checkpoint for Tol0.8_Wt0.25. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.7_Wt0.05 ---\n",
            "Found checkpoint for Tol0.7_Wt0.05. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.7_Wt0.1 ---\n",
            "Found checkpoint for Tol0.7_Wt0.1. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.7_Wt0.15 ---\n",
            "Found checkpoint for Tol0.7_Wt0.15. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.7_Wt0.2 ---\n",
            "Found checkpoint for Tol0.7_Wt0.2. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "--- Checking Config: Tol0.7_Wt0.25 ---\n",
            "Found checkpoint for Tol0.7_Wt0.25. Loading...\n",
            "Loaded successfully. Skipping re-computation.\n",
            "\n",
            "======================================================================\n",
            "All 10 combinations completed/loaded successfully!\n",
            "Keys available in results_storage: ['Tol0.9_Wt0.05', 'Tol0.9_Wt0.1', 'Tol0.9_Wt0.15', 'Tol0.9_Wt0.2', 'Tol0.9_Wt0.25', 'Tol0.8_Wt0.05', 'Tol0.8_Wt0.1', 'Tol0.8_Wt0.15', 'Tol0.8_Wt0.2', 'Tol0.8_Wt0.25', 'Tol0.7_Wt0.05', 'Tol0.7_Wt0.1', 'Tol0.7_Wt0.15', 'Tol0.7_Wt0.2', 'Tol0.7_Wt0.25']\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# Cell 4: Multi-Cycle Experiments with Checkpointing\n",
        "# ======================================================\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Define Checkpoint Directory\n",
        "#    This will create a folder \"Starvation_Checkpoints\" in your MyDrive.\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Starvation_Checkpoints1\"\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR)\n",
        "    print(f\"Created checkpoint directory: {CHECKPOINT_DIR}\")\n",
        "else:\n",
        "    print(f\"Using existing checkpoint directory: {CHECKPOINT_DIR}\")\n",
        "\n",
        "results_storage = {}\n",
        "\n",
        "# Experiment Configurations\n",
        "rtw_target = 0.05\n",
        "bv_tolerances = [0.90, 0.80, 0.70]\n",
        "starvation_weights = [0.05, 0.10, 0.15, 0.20, 0.25]\n",
        "\n",
        "print(f\"Starting Starvation Sensitivity Experiments at RTW={rtw_target*100}%...\")\n",
        "print(f\"Cycles: {NUM_CYCLES}, Runs per Cycle: {RUNS_PER_CYCLE}\")\n",
        "\n",
        "for tol in bv_tolerances:\n",
        "    for weight in starvation_weights:\n",
        "        # Create unique label\n",
        "        config_label = f\"Tol{tol}_Wt{weight}\"\n",
        "        checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"{config_label}.pkl\")\n",
        "\n",
        "        print(f\"\\n--- Checking Config: {config_label} ---\")\n",
        "\n",
        "        # --- CHECKPOINT LOGIC ---\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            print(f\"Found checkpoint for {config_label}. Loading...\")\n",
        "            try:\n",
        "                with open(checkpoint_file, 'rb') as f:\n",
        "                    cycle_results = pickle.load(f)\n",
        "                # Add to current results storage so plotting works later\n",
        "                results_storage[config_label] = cycle_results\n",
        "                print(\"Loaded successfully. Skipping re-computation.\")\n",
        "                continue # Skip to next iteration\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading checkpoint (file might be corrupted): {e}\")\n",
        "                print(\"Re-running this configuration...\")\n",
        "\n",
        "        # --- EXECUTION LOGIC (If no checkpoint) ---\n",
        "        print(f\"No checkpoint found. Running Hist-RTSGA...\")\n",
        "\n",
        "        # Update GA Parameters\n",
        "        current_ga_params = GA_PARAMS.copy()\n",
        "        current_ga_params['bv_tolerance'] = tol\n",
        "        current_ga_params['starvation_weight'] = weight\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the experiment\n",
        "        cycle_results = run_hist_rtsga_cycles(\n",
        "            rtw_ratio=rtw_target,\n",
        "            data_maps=data_maps,\n",
        "            ga_params=current_ga_params,\n",
        "            num_cycles=NUM_CYCLES,\n",
        "            runs_per_cycle=RUNS_PER_CYCLE\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Completed {config_label} in {elapsed:.2f}s\")\n",
        "\n",
        "        # Save to memory\n",
        "        results_storage[config_label] = cycle_results\n",
        "\n",
        "        # Save to Drive (Checkpoint)\n",
        "        try:\n",
        "            with open(checkpoint_file, 'wb') as f:\n",
        "                pickle.dump(cycle_results, f)\n",
        "            print(f\"Checkpoint saved to: {checkpoint_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Could not save checkpoint: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"All 10 combinations completed/loaded successfully!\")\n",
        "print(\"Keys available in results_storage:\", list(results_storage.keys()))\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoO_PJI-0Bv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcbd2090-e3ef-4b8f-d25d-9eee1cd4990f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using results directory: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables\n",
            "\n",
            "Generating Selection History Files...\n",
            "============================================================\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.9_Wt0.05.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.9_Wt0.1.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.9_Wt0.15.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.9_Wt0.2.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.9_Wt0.25.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.8_Wt0.05.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.8_Wt0.1.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.8_Wt0.15.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.8_Wt0.2.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.8_Wt0.25.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.7_Wt0.05.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.7_Wt0.1.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.7_Wt0.15.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.7_Wt0.2.xlsx\n",
            "Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_Tol0.7_Wt0.25.xlsx\n",
            "\n",
            "============================================================\n",
            "Saved Consolidated File: /content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables/selection_history_ALL_COMBINED.xlsx\n",
            "All selection history tables created successfully!\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# Cell 5: Create Selection History Tables (Save to Drive & Consolidated)\n",
        "# ======================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 1. Define Directory in Google Drive for Results\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/RTS/Starvation_output_half_size/history_tables\"\n",
        "\n",
        "if not os.path.exists(RESULTS_DIR):\n",
        "    os.makedirs(RESULTS_DIR)\n",
        "    print(f\"Created results directory: {RESULTS_DIR}\")\n",
        "else:\n",
        "    print(f\"Using results directory: {RESULTS_DIR}\")\n",
        "\n",
        "def create_selection_history_table(cycles_data, config_name):\n",
        "    \"\"\"\n",
        "    Create table showing (req_id, BV) pairs for the best run in each cycle.\n",
        "    Adapted for Starvation Sensitivity Analysis structure.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    # We are processing a single configuration's list of cycles\n",
        "    row = {'Configuration': config_name}\n",
        "\n",
        "    for cycle_result in cycles_data:\n",
        "        cycle_idx = cycle_result['cycle_idx']\n",
        "        best_reqs = cycle_result['best_selected_reqs']\n",
        "\n",
        "        # Get (req_id, BV) pairs using the global data_maps\n",
        "        req_bv_pairs = [(req, data_maps['req_to_bv'][req]) for req in best_reqs]\n",
        "\n",
        "        # Format as string: \"(req1, 10), (req5, 50)\"\n",
        "        req_bv_str = ', '.join([f\"({r},{bv})\" for r, bv in req_bv_pairs])\n",
        "\n",
        "        row[f'C{cycle_idx}'] = req_bv_str\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "    df_history = pd.DataFrame(rows)\n",
        "    return df_history\n",
        "\n",
        "# 2. Main Processing Loop\n",
        "print(\"\\nGenerating Selection History Files...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# List to collect all dataframes for the consolidated file\n",
        "all_configs_history_list = []\n",
        "\n",
        "for config_label in results_storage.keys():\n",
        "    # Generate table for this specific config\n",
        "    history_table = create_selection_history_table(results_storage[config_label], config_label)\n",
        "\n",
        "    # Append to list for consolidated file\n",
        "    all_configs_history_list.append(history_table)\n",
        "\n",
        "    # Save individual file to Drive\n",
        "    history_filename = f'selection_history_{config_label}.xlsx'\n",
        "    full_path = os.path.join(RESULTS_DIR, history_filename)\n",
        "\n",
        "    history_table.to_excel(full_path, index=False)\n",
        "    print(f\"Saved Individual: {full_path}\")\n",
        "\n",
        "# 3. Save Consolidated File containing ALL combinations\n",
        "if all_configs_history_list:\n",
        "    combined_history_df = pd.concat(all_configs_history_list, ignore_index=True)\n",
        "\n",
        "    combined_filename = \"selection_history_ALL_COMBINED.xlsx\"\n",
        "    combined_full_path = os.path.join(RESULTS_DIR, combined_filename)\n",
        "\n",
        "    combined_history_df.to_excel(combined_full_path, index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Saved Consolidated File: {combined_full_path}\")\n",
        "\n",
        "print(\"All selection history tables created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf0shTO10Bv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e37e83-36a9-4e03-931a-7d6ddd7b59b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Statistics...\n",
            "============================================================\n",
            "Processing: Tol0.9_Wt0.05\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.9_Wt0.05.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                     C5                     C6\n",
            "Tol0.9_Wt0.05 (739.00, 711.00, 6.56) (547.89, 502.11, 12.39) (473.34, 429.37, 14.08) (421.44, 410.94, 2.09) (393.92, 365.86, 8.02) (351.95, 326.13, 8.24)\n",
            "----------------------------------------\n",
            "Processing: Tol0.9_Wt0.1\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.9_Wt0.1.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                      C5                      C6\n",
            " Tol0.9_Wt0.1 (719.00, 711.00, 1.44) (560.11, 519.93, 11.47) (471.55, 439.14, 11.12) (417.42, 398.96, 6.31) (385.42, 346.53, 12.19) (367.36, 323.36, 14.32)\n",
            "----------------------------------------\n",
            "Processing: Tol0.9_Wt0.15\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.9_Wt0.15.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                     C5                      C6\n",
            "Tol0.9_Wt0.15 (739.00, 711.00, 6.51) (547.89, 502.11, 13.24) (472.55, 429.37, 14.53) (420.84, 410.94, 2.25) (386.22, 354.14, 9.02) (368.67, 325.83, 14.18)\n",
            "----------------------------------------\n",
            "Processing: Tol0.9_Wt0.2\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.9_Wt0.2.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                     C2                     C3                     C4                     C5                     C6\n",
            " Tol0.9_Wt0.2 (735.00, 711.00, 6.07) (552.58, 519.93, 9.28) (464.46, 451.30, 2.70) (426.26, 401.68, 5.90) (396.17, 365.86, 7.73) (354.55, 341.62, 3.54)\n",
            "----------------------------------------\n",
            "Processing: Tol0.9_Wt0.25\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.9_Wt0.25.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                      C5                      C6\n",
            "Tol0.9_Wt0.25 (739.00, 711.00, 8.42) (547.89, 502.11, 14.68) (466.55, 429.37, 14.91) (413.17, 393.37, 6.86) (398.28, 353.48, 14.63) (366.29, 332.31, 10.18)\n",
            "----------------------------------------\n",
            "Processing: Tol0.8_Wt0.05\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.8_Wt0.05.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                      C4                      C5                     C6\n",
            "Tol0.8_Wt0.05 (739.00, 711.00, 8.01) (534.45, 502.11, 11.14) (474.39, 444.21, 10.31) (425.26, 381.38, 13.99) (387.51, 338.61, 16.19) (362.25, 332.31, 7.97)\n",
            "----------------------------------------\n",
            "Processing: Tol0.8_Wt0.1\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.8_Wt0.1.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                      C5                     C6\n",
            " Tol0.8_Wt0.1 (719.00, 711.00, 2.04) (557.77, 519.93, 10.91) (468.28, 433.49, 11.79) (425.02, 398.96, 8.39) (393.76, 350.33, 12.65) (359.36, 337.55, 6.88)\n",
            "----------------------------------------\n",
            "Processing: Tol0.8_Wt0.15\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.8_Wt0.15.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                     C5                     C6\n",
            "Tol0.8_Wt0.15 (735.00, 711.00, 5.99) (550.96, 519.93, 11.68) (468.54, 431.48, 15.60) (414.03, 389.02, 5.88) (389.90, 373.61, 4.14) (351.96, 332.31, 4.68)\n",
            "----------------------------------------\n",
            "Processing: Tol0.8_Wt0.2\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.8_Wt0.2.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                      C5                     C6\n",
            " Tol0.8_Wt0.2 (735.00, 711.00, 8.18) (548.96, 519.93, 10.51) (463.17, 413.91, 14.73) (433.77, 424.55, 1.66) (386.33, 353.26, 11.45) (363.69, 343.83, 6.56)\n",
            "----------------------------------------\n",
            "Processing: Tol0.8_Wt0.25\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.8_Wt0.25.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                      C5                     C6\n",
            "Tol0.8_Wt0.25 (719.00, 711.00, 1.47) (560.93, 519.93, 11.65) (470.71, 436.32, 12.99) (422.44, 410.94, 2.57) (391.61, 338.61, 14.87) (352.51, 321.86, 9.24)\n",
            "----------------------------------------\n",
            "Processing: Tol0.7_Wt0.05\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.7_Wt0.05.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                     C3                     C4                     C5                     C6\n",
            "Tol0.7_Wt0.05 (735.00, 711.00, 4.48) (560.96, 519.93, 13.88) (457.34, 439.88, 3.97) (430.60, 423.26, 1.55) (391.70, 369.24, 7.15) (348.92, 326.13, 8.29)\n",
            "----------------------------------------\n",
            "Processing: Tol0.7_Wt0.1\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.7_Wt0.1.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                     C2                      C3                      C4                     C5                     C6\n",
            " Tol0.7_Wt0.1 (739.00, 711.00, 6.62) (531.96, 502.11, 8.73) (466.23, 424.30, 13.63) (417.67, 383.54, 12.17) (376.47, 354.14, 6.90) (377.02, 343.25, 9.22)\n",
            "----------------------------------------\n",
            "Processing: Tol0.7_Wt0.15\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.7_Wt0.15.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                     C3                     C4                      C5                     C6\n",
            "Tol0.7_Wt0.15 (739.00, 711.00, 8.83) (534.18, 502.11, 10.48) (457.66, 448.37, 2.21) (420.23, 410.48, 2.35) (409.54, 342.55, 21.41) (368.93, 337.33, 8.00)\n",
            "----------------------------------------\n",
            "Processing: Tol0.7_Wt0.2\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.7_Wt0.2.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                      C3                     C4                     C5                     C6\n",
            " Tol0.7_Wt0.2 (739.00, 711.00, 8.08) (547.89, 502.11, 13.52) (471.34, 429.37, 13.91) (407.74, 393.37, 5.02) (388.66, 376.06, 3.58) (363.77, 342.44, 6.02)\n",
            "----------------------------------------\n",
            "Processing: Tol0.7_Wt0.25\n",
            "  -> Saved Individual: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_Tol0.7_Wt0.25.xlsx\n",
            "  -> Preview (BV):\n",
            "Configuration                     C1                      C2                     C3                     C4                     C5                     C6\n",
            "Tol0.7_Wt0.25 (739.00, 711.00, 8.16) (535.96, 502.11, 10.16) (461.10, 451.30, 2.39) (446.47, 404.23, 9.70) (378.35, 356.03, 7.00) (345.98, 317.05, 8.12)\n",
            "----------------------------------------\n",
            "\n",
            "============================================================\n",
            "Saved Consolidated File: /content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics/summary_statistics_ALL_COMBINED.xlsx\n",
            "\n",
            "All summary statistics calculated and saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# Cell 6: Calculate Summary Statistics (Save to Drive & Consolidated)\n",
        "# ======================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 1. Define Directory in Google Drive\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/RTS/Starvation_output_half_size/Statistics\"\n",
        "\n",
        "if not os.path.exists(RESULTS_DIR):\n",
        "    os.makedirs(RESULTS_DIR)\n",
        "    print(f\"Using results directory: {RESULTS_DIR}\")\n",
        "\n",
        "# Ensure counts exist in data_maps to avoid KeyError\n",
        "if 'n_reqs' not in data_maps:\n",
        "    data_maps['n_reqs'] = len(data_maps['all_req_ids'])\n",
        "if 'n_tests' not in data_maps:\n",
        "    data_maps['n_tests'] = len(data_maps['test_to_time'])\n",
        "\n",
        "def calculate_cycle_statistics(cycles_data, data_maps):\n",
        "    \"\"\"\n",
        "    Calculate mean, std, median for BV, req coverage, and test coverage.\n",
        "    Input: cycles_data is a LIST of cycle results for one configuration.\n",
        "    \"\"\"\n",
        "    method_stats = []\n",
        "\n",
        "    for cycle_result in cycles_data:\n",
        "        cycle_idx = cycle_result['cycle_idx']\n",
        "        all_runs = cycle_result['all_runs']\n",
        "\n",
        "        # Extract metrics from all 30 runs\n",
        "        bv_values = []\n",
        "        req_cov_values = []\n",
        "        test_cov_values = []\n",
        "\n",
        "        for run in all_runs:\n",
        "            selected_reqs = run['selected_reqs']\n",
        "\n",
        "            # BV\n",
        "            total_bv = run['fitness']\n",
        "            bv_values.append(total_bv)\n",
        "\n",
        "            # Requirement coverage\n",
        "            req_cov = (len(selected_reqs) / data_maps['n_reqs']) * 100\n",
        "            req_cov_values.append(req_cov)\n",
        "\n",
        "            # Test coverage\n",
        "            covered_tests = set()\n",
        "            for req in selected_reqs:\n",
        "                covered_tests |= data_maps['req_to_tests'].get(req, set())\n",
        "            test_cov = (len(covered_tests) / data_maps['n_tests']) * 100\n",
        "            test_cov_values.append(test_cov)\n",
        "\n",
        "        # Calculate statistics\n",
        "        cycle_stats = {\n",
        "            'cycle': cycle_idx,\n",
        "            'bv_mean': np.mean(bv_values),\n",
        "            'bv_std': np.std(bv_values),\n",
        "            'bv_median': np.median(bv_values),\n",
        "            'bv_max': np.max(bv_values),\n",
        "            'bv_min': np.min(bv_values),\n",
        "            'req_cov_mean': np.mean(req_cov_values),\n",
        "            'req_cov_std': np.std(req_cov_values),\n",
        "            'req_cov_median': np.median(req_cov_values),\n",
        "            'req_cov_max': np.max(req_cov_values),\n",
        "            'req_cov_min': np.min(req_cov_values),\n",
        "            'test_cov_mean': np.mean(test_cov_values),\n",
        "            'test_cov_std': np.std(test_cov_values),\n",
        "            'test_cov_median': np.median(test_cov_values),\n",
        "            'test_cov_max': np.max(test_cov_values),\n",
        "            'test_cov_min': np.min(test_cov_values)\n",
        "        }\n",
        "        method_stats.append(cycle_stats)\n",
        "\n",
        "    return pd.DataFrame(method_stats)\n",
        "\n",
        "\n",
        "def create_summary_table(df_stats, config_name, metric='bv'):\n",
        "    \"\"\"Create table with (max, min, std) for each cycle for the specific config\"\"\"\n",
        "    rows = []\n",
        "\n",
        "    # Row for this specific configuration\n",
        "    row = {'Configuration': config_name}\n",
        "\n",
        "    for _, cycle_row in df_stats.iterrows():\n",
        "        cycle_idx = int(cycle_row['cycle'])\n",
        "        max_val = cycle_row[f'{metric}_max']\n",
        "        min_val = cycle_row[f'{metric}_min']\n",
        "        std_val = cycle_row[f'{metric}_std']\n",
        "\n",
        "        row[f'C{cycle_idx}'] = f\"({max_val:.2f}, {min_val:.2f}, {std_val:.2f})\"\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# Calculate and save statistics for each configuration\n",
        "all_statistics = {}\n",
        "\n",
        "# Lists to hold dataframes for the FINAL consolidated file\n",
        "consolidated_bv = []\n",
        "consolidated_req = []\n",
        "consolidated_test = []\n",
        "\n",
        "print(\"Calculating Statistics...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for config_label in results_storage.keys():\n",
        "    print(f\"Processing: {config_label}\")\n",
        "\n",
        "    # Calculate stats for this specific configuration list\n",
        "    df_stats = calculate_cycle_statistics(results_storage[config_label], data_maps)\n",
        "    all_statistics[config_label] = df_stats\n",
        "\n",
        "    # Create summary tables for this config\n",
        "    bv_table = create_summary_table(df_stats, config_label, 'bv')\n",
        "    req_cov_table = create_summary_table(df_stats, config_label, 'req_cov')\n",
        "    test_cov_table = create_summary_table(df_stats, config_label, 'test_cov')\n",
        "\n",
        "    # Append to consolidated lists\n",
        "    consolidated_bv.append(bv_table)\n",
        "    consolidated_req.append(req_cov_table)\n",
        "    consolidated_test.append(test_cov_table)\n",
        "\n",
        "    # Save INDIVIDUAL file to Drive\n",
        "    filename = f'summary_statistics_{config_label}.xlsx'\n",
        "    full_path = os.path.join(RESULTS_DIR, filename)\n",
        "\n",
        "    with pd.ExcelWriter(full_path, engine='openpyxl') as writer:\n",
        "        bv_table.to_excel(writer, sheet_name='BV_Summary', index=False)\n",
        "        req_cov_table.to_excel(writer, sheet_name='Req_Cov_Summary', index=False)\n",
        "        test_cov_table.to_excel(writer, sheet_name='Test_Cov_Summary', index=False)\n",
        "        df_stats.to_excel(writer, sheet_name='Full_Statistics', index=False)\n",
        "\n",
        "    print(f\"  -> Saved Individual: {full_path}\")\n",
        "\n",
        "    # Display BV summary (first 6 cycles)\n",
        "    display_cols = ['Configuration'] + [f'C{i}' for i in range(1, min(7, NUM_CYCLES + 1))]\n",
        "    print(f\"  -> Preview (BV):\")\n",
        "    print(bv_table[display_cols].to_string(index=False))\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# 2. Save CONSOLIDATED file containing ALL combinations\n",
        "if consolidated_bv:\n",
        "    combined_bv_df = pd.concat(consolidated_bv, ignore_index=True)\n",
        "    combined_req_df = pd.concat(consolidated_req, ignore_index=True)\n",
        "    combined_test_df = pd.concat(consolidated_test, ignore_index=True)\n",
        "\n",
        "    combined_filename = \"summary_statistics_ALL_COMBINED.xlsx\"\n",
        "    combined_path = os.path.join(RESULTS_DIR, combined_filename)\n",
        "\n",
        "    with pd.ExcelWriter(combined_path, engine='openpyxl') as writer:\n",
        "        combined_bv_df.to_excel(writer, sheet_name='All_BV_Summaries', index=False)\n",
        "        combined_req_df.to_excel(writer, sheet_name='All_Req_Summaries', index=False)\n",
        "        combined_test_df.to_excel(writer, sheet_name='All_Test_Summaries', index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Saved Consolidated File: {combined_path}\")\n",
        "\n",
        "print(\"\\nAll summary statistics calculated and saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDDa1KMp0Bv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e99e48-2e1b-4849-f34d-6319e76939e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ 'results_storage' found in memory (15 configs).\n",
            "⚠️ 'all_statistics' missing or empty. Recalculating...\n",
            "  -> Statistics recalculated for 15 configs.\n",
            "✅ Loaded RTSGA Baseline from: /content/drive/MyDrive/RTS/Starvation_output_half_size/summary_statistics_RTSGA_only_RTW_5%.xlsx\n",
            "   Data shape: (20, 16)\n",
            "   Columns: ['cycle', 'bv_mean', 'bv_std', 'bv_median', 'bv_max', 'bv_min', 'req_cov_mean', 'req_cov_std', 'req_cov_median', 'req_cov_max', 'req_cov_min', 'test_cov_mean', 'test_cov_std', 'test_cov_median', 'test_cov_max', 'test_cov_min']\n",
            "\n",
            "Generating Comparison Plots...\n",
            "  Processing: Tol0.9_Wt0.05\n",
            "  Processing: Tol0.9_Wt0.1\n",
            "  Processing: Tol0.9_Wt0.15\n",
            "  Processing: Tol0.9_Wt0.2\n",
            "  Processing: Tol0.9_Wt0.25\n",
            "  Processing: Tol0.8_Wt0.05\n",
            "  Processing: Tol0.8_Wt0.1\n",
            "  Processing: Tol0.8_Wt0.15\n",
            "  Processing: Tol0.8_Wt0.2\n",
            "  Processing: Tol0.8_Wt0.25\n",
            "  Processing: Tol0.7_Wt0.05\n",
            "  Processing: Tol0.7_Wt0.1\n",
            "  Processing: Tol0.7_Wt0.15\n",
            "  Processing: Tol0.7_Wt0.2\n",
            "  Processing: Tol0.7_Wt0.25\n",
            "\n",
            "============================================================\n",
            "✅ Comparison plots saved to: /content/drive/MyDrive/RTS/Starvation_output_half_size/comparison_plots\n",
            "✅ Combined PDF saved as: /content/drive/MyDrive/RTS/Starvation_output_half_size/comparison_plots/All_Comparisons_Combined.pdf\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# Cell 7: Create Comparative Plots (Fixed Empty Statistics Issue)\n",
        "# ======================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.backends.backend_pdf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 1. Force Mount Google Drive\n",
        "# ------------------------------------------------------\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print(\"Warning: Could not mount Drive. Files will be saved locally.\")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 2. Configuration & Directories\n",
        "# ------------------------------------------------------\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Starvation_Checkpoints1\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/RTS/Starvation_output_half_size/comparison_plots\"\n",
        "\n",
        "for d in [CHECKPOINT_DIR, RESULTS_DIR]:\n",
        "    if not os.path.exists(d):\n",
        "        os.makedirs(d)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 3. Auto-Recovery: Reload Results if Missing or Empty\n",
        "# ------------------------------------------------------\n",
        "# Reload if variable missing OR if it's an empty dictionary\n",
        "if 'results_storage' not in globals() or not results_storage:\n",
        "    print(\"⚠️ 'results_storage' missing or empty. Reloading from checkpoints...\")\n",
        "    results_storage = {}\n",
        "    if os.path.exists(CHECKPOINT_DIR):\n",
        "        for f in os.listdir(CHECKPOINT_DIR):\n",
        "            if f.endswith(\".pkl\"):\n",
        "                label = f.replace(\".pkl\", \"\")\n",
        "                try:\n",
        "                    with open(os.path.join(CHECKPOINT_DIR, f), 'rb') as pkl_file:\n",
        "                        results_storage[label] = pickle.load(pkl_file)\n",
        "                except Exception as e:\n",
        "                    print(f\"  - Failed to load {f}: {e}\")\n",
        "    print(f\"  -> Recovered {len(results_storage)} configurations.\")\n",
        "else:\n",
        "    print(f\"✅ 'results_storage' found in memory ({len(results_storage)} configs).\")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 4. Auto-Recovery: Recalculate Statistics\n",
        "# ------------------------------------------------------\n",
        "# FIXED: Now checks if all_statistics is empty/falsey\n",
        "if 'all_statistics' not in globals() or not all_statistics:\n",
        "    print(\"⚠️ 'all_statistics' missing or empty. Recalculating...\")\n",
        "\n",
        "    if 'data_maps' not in globals():\n",
        "        print(\"❌ CRITICAL ERROR: 'data_maps' is missing. Please re-run Cells 1-3 to load your dataset.\")\n",
        "    else:\n",
        "        # Helper to calculate stats\n",
        "        def calc_stats_internal(cycles_data):\n",
        "            method_stats = []\n",
        "            for cycle_res in cycles_data:\n",
        "                c_idx = cycle_res['cycle_idx']\n",
        "                runs = cycle_res['all_runs']\n",
        "                bv_vals = [r['fitness'] for r in runs]\n",
        "                req_vals = [(len(r['selected_reqs'])/data_maps['n_reqs'])*100 for r in runs]\n",
        "                test_vals = []\n",
        "                for r in runs:\n",
        "                    cov_t = set()\n",
        "                    for req in r['selected_reqs']:\n",
        "                        cov_t |= data_maps['req_to_tests'].get(req, set())\n",
        "                    test_vals.append((len(cov_t)/data_maps['n_tests'])*100)\n",
        "\n",
        "                method_stats.append({\n",
        "                    'cycle': c_idx,\n",
        "                    'bv_mean': np.mean(bv_vals), 'bv_std': np.std(bv_vals),\n",
        "                    'req_cov_mean': np.mean(req_vals), 'req_cov_std': np.std(req_vals),\n",
        "                    'test_cov_mean': np.mean(test_vals), 'test_cov_std': np.std(test_vals)\n",
        "                })\n",
        "            return pd.DataFrame(method_stats)\n",
        "\n",
        "        all_statistics = {}\n",
        "        for label, data in results_storage.items():\n",
        "            all_statistics[label] = calc_stats_internal(data)\n",
        "        print(f\"  -> Statistics recalculated for {len(all_statistics)} configs.\")\n",
        "else:\n",
        "    print(f\"✅ 'all_statistics' found in memory ({len(all_statistics)} configs).\")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 5. Load RTSGA Baseline (Updated Path)\n",
        "# ------------------------------------------------------\n",
        "rtsga_df = None\n",
        "found_file = None\n",
        "\n",
        "# UPDATED: Direct path to your RTSGA Excel file\n",
        "RTSGA_FILE_PATH = '/content/drive/MyDrive/RTS/Starvation_output_half_size/summary_statistics_RTSGA_only_RTW_5%.xlsx'\n",
        "\n",
        "# Search candidates (prioritize the new file first)\n",
        "candidates = [\n",
        "    RTSGA_FILE_PATH,                                             # Your new RTSGA file (PRIORITY)\n",
        "    os.path.join(RESULTS_DIR, 'summary_statistics_RTSGA_only_RTW_5%.xlsx'),  # Alternative in RESULTS_DIR\n",
        "    os.path.join(RESULTS_DIR, 'summary_statistics_RTW_5%.xlsx'), # Old naming\n",
        "    'summary_statistics_RTSGA_only_RTW_5%.xlsx'                  # Local fallback\n",
        "]\n",
        "\n",
        "for path in candidates:\n",
        "    if os.path.exists(path):\n",
        "        found_file = path\n",
        "        break\n",
        "\n",
        "if found_file:\n",
        "    try:\n",
        "        # Load the RTSGA_Full sheet\n",
        "        rtsga_df = pd.read_excel(found_file, sheet_name='RTSGA_Full')\n",
        "        print(f\"✅ Loaded RTSGA Baseline from: {found_file}\")\n",
        "        print(f\"   Data shape: {rtsga_df.shape}\")\n",
        "        print(f\"   Columns: {rtsga_df.columns.tolist()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading file {found_file}: {e}\")\n",
        "        rtsga_df = None\n",
        "else:\n",
        "    print(\"⚠️ WARNING: RTSGA baseline file not found at any of these locations:\")\n",
        "    for path in candidates:\n",
        "        print(f\"  - {path}\")\n",
        "    print(\"\\nPlots will be single-bar only (no baseline comparison).\")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 6. Plotting Logic\n",
        "# ------------------------------------------------------\n",
        "def create_comparison_plot(df_hist, df_rtsga, config_name, metric_name, metric_key, ylabel):\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "    cycles = df_hist['cycle'].values\n",
        "    hist_means = df_hist[f'{metric_key}_mean'].values\n",
        "    hist_stds = df_hist[f'{metric_key}_std'].values\n",
        "    x = np.arange(len(cycles))\n",
        "    width = 0.35\n",
        "\n",
        "    # Bar 1: Current Config (Blue)\n",
        "    ax.bar(x - width/2, hist_means, width, label=config_name,\n",
        "           color='#4A90E2', alpha=0.95, yerr=hist_stds, capsize=4)\n",
        "\n",
        "    # Bar 2: RTSGA Baseline (Orange)\n",
        "    if df_rtsga is not None:\n",
        "        limit = min(len(cycles), len(df_rtsga))\n",
        "        r_means = df_rtsga[f'{metric_key}_mean'].values[:limit]\n",
        "        r_stds = df_rtsga[f'{metric_key}_std'].values[:limit]\n",
        "        ax.bar(x[:limit] + width/2, r_means, width, label='RTSGA (Baseline)',\n",
        "               color='#FF8C42', alpha=0.95, yerr=r_stds, capsize=4)\n",
        "\n",
        "    ax.set_xlabel('Regression Cycle (k)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'{metric_name}: {config_name} vs RTSGA', fontsize=14, fontweight='bold', pad=15)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([f'C{int(c)}' for c in cycles], fontsize=10)\n",
        "    ax.legend(loc='upper right', fontsize=11)\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 7. Generate & Save\n",
        "# ------------------------------------------------------\n",
        "print(\"\\nGenerating Comparison Plots...\")\n",
        "pdf_path = os.path.join(RESULTS_DIR, \"All_Comparisons_Combined.pdf\")\n",
        "pdf_pages = matplotlib.backends.backend_pdf.PdfPages(pdf_path)\n",
        "\n",
        "if not all_statistics:\n",
        "    print(\"❌ ERROR: Still no statistics available. Please check if checkpoints exist in Drive.\")\n",
        "else:\n",
        "    for label in all_statistics.keys():\n",
        "        print(f\"  Processing: {label}\")\n",
        "        df_stats = all_statistics[label]\n",
        "\n",
        "        # BV\n",
        "        fig1 = create_comparison_plot(df_stats, rtsga_df, label, 'Business Value', 'bv', 'Mean BV')\n",
        "        fig1.savefig(os.path.join(RESULTS_DIR, f'BV_Compare_{label}.png'), dpi=300)\n",
        "        pdf_pages.savefig(fig1)\n",
        "        plt.close(fig1)\n",
        "\n",
        "        # Req Cov\n",
        "        fig2 = create_comparison_plot(df_stats, rtsga_df, label, 'Req Coverage', 'req_cov', 'Mean Req Coverage (%)')\n",
        "        fig2.savefig(os.path.join(RESULTS_DIR, f'ReqCov_Compare_{label}.png'), dpi=300)\n",
        "        pdf_pages.savefig(fig2)\n",
        "        plt.close(fig2)\n",
        "\n",
        "        # Test Cov\n",
        "        fig3 = create_comparison_plot(df_stats, rtsga_df, label, 'Test Coverage', 'test_cov', 'Mean Test Coverage (%)')\n",
        "        fig3.savefig(os.path.join(RESULTS_DIR, f'TestCov_Compare_{label}.png'), dpi=300)\n",
        "        pdf_pages.savefig(fig3)\n",
        "        plt.close(fig3)\n",
        "\n",
        "    pdf_pages.close()\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"✅ Comparison plots saved to: {RESULTS_DIR}\")\n",
        "    print(f\"✅ Combined PDF saved as: {pdf_path}\")\n",
        "    print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}